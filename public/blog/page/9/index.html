
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Naildrivin' &#10106;</title>
  <meta name="author" content="David Bryant Copeland">

  
  <meta name="description" content="Following on from my post on Gliffy&#8217;s blog&#8230; On more than a few occasions, I&#8217;ve been faced with making significant refactorings to &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://www.naildrivin5.com/blog/page/9/index.html">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Naildrivin' &#10106;" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-10518541-5']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


  <meta name="google-site-verification" content="h_yTpXa6N3ebHj8DYmgX4lIFGHBW1NtGMVHfXuu7i_4" />
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Naildrivin' &#10106;</a></h1>
  
    <h2>&#10144; Website of David Bryant Copeland</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:www.naildrivin5.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="http://www.awesomecommandlineapps.com">My Book!</a></li>
  <li><a href="/scalatour">Scala Tour</a></li>
  <li><a href="/talks">Talks</a></li>
  <li><a href="/blog/archives">Blog Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/09/08/didnt-do-test-driven-design-record-your-test-cases-later.html">Didn&#8217;t Do Test-Driven Design? Record Your Test Cases Later</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-09-08T00:00:00-04:00" pubdate data-updated="true">Sep 8<span>th</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><i>Following on from <a href="http://www.gliffy.com/blog/2008/09/09/testing-gliffy-without-testers/">my post on Gliffy&#8217;s blog&#8230;</a></i></p>
<p>
On more than a few occasions, I&#8217;ve been faced with making significant refactorings to an existing application.   These are things where we need to overhaul an architectural component without breaking anything, or changing the application&#8217;s features.  For an applicaiton without any test cases, this is not only scary, but ill-advised.  </p>
<p>
I believe this is the primary reason that development shops hang on to out-dated technology.  I got a job at a web development shop after <b>four</b> years of doing nothing but Swing and J2EE.  My last experience with Java web development was Servlets, JSPs and taglibs.  This company was <b>still</b> using these as the primary components of their architecture.  No Struts, no Spring, no SEAM.  Why?  One reason was that they had <b>no</b> test infrastructure and therefore <b>not</b> ability to refactor anything.
</p>
<h3>Doing it anyway</h3>
<p>
Nevertheless, sometimes the benefits outweigh the costs and you <b>really</b> need to make a change.  At <a href="http://www.gliffy.com">Gliffy</a>, I was hired to create an API to integrate editing Gliffy diagrams into the workflow of other applications.  After a review of their code and architecture, the principals and I decided that the database layer needed an overhaul.  It was using JDBC/SQL and had become difficult to change (especially to the new guy: me).  I suggested moving to the  <a href="http://www.naildrivin5.com/daveblog5000/?tag=jpa">Java Persistence Architecture</a> (backed by Hibernate), and they agreed.  Only problem was how to make sure I didn&#8217;t break anything.  They didn&#8217;t have automated tests, and I was totally new to the application environment.
</p>
<p>
They <b>did</b> have test scripts for testers to follow that would hit various parts of the application.  Coming from my previous enviornment, that in and of itself was amazing.  Since the application communicates with the server entirely via <tt>HTTP POST</tt>, and recieves mostly XML back, I figured I could manually execute the tests and record them in a way so they could be played back later as regression tests. 
</p>
<h3>Recording Tests</h3>
<p>
This is suprisingly easy thanks to the filtering features of the Servlet specification:
<div>
  <pre>
    <code class='java'>&lt;filter&gt;
  &lt;filter-name&gt;recorder&lt;/filter-name&gt;
  &lt;filter-class&gt;com.gliffy.test.online.RecordServletFilter&lt;/filter-class&gt;
&lt;/filter&gt;

&lt;!-- ... --&gt;

&lt;filter-mapping&gt;
  &lt;filter-name&gt;recorder&lt;/filter-name&gt;
  &lt;url-pattern&gt;/*&lt;/url-pattern&gt;
&lt;/filter-mapping&gt;</code>
  </pre>
</div>


The filter code is bit more complex, because I had to create proxy classes for <tt>HttpServletRequest</tt> and <tt>HttpServletResponse</tt>.  Here&#8217;s an overview of how everything fits together:
<a href="http://www.gliffy.com/pubdoc/1487626/L.jpg"><img src="http://www.gliffy.com/pubdoc/1487626/S.jpg" border=0></a>
</p>
<p>The request proxy had to read everything from the requests input stream, save it, and send a new stream that would output the same data to the caller.  It had to do the same thing with the <tt>Reader</tt>.  I&#8217;m sure it&#8217;s an error to use both in the same request, and Gliffy&#8217;s code didn&#8217;t do that, so this worked well.
<div>
  <pre>
    <code class='java'>private class RecordingServletRequest extends javax.servlet.http.HttpServletRequestWrapper
{
    BufferedReader reader = null;
    ServletInputStream inputStream = null;

    String readerContent = null;
    byte inputStreamContent[] = null;

    public RecordingServletRequest(HttpServletRequest r) { super(r); }

    public BufferedReader getReader()
        throws IOException
    {
        if (reader == null)
        {
            StringWriter writer = new StringWriter();
            BufferedReader superReader = super.getReader();
            int ch = superReader.read();
            while (ch != -1)
            {
                writer.write(ch);
                ch = superReader.read();
            }
            readerContent = writer.toString();
            return new BufferedReader(new StringReader(readerContent));
        }
        return reader;
    }

    public ServletInputStream getInputStream()
        throws IOException
    {
        if (inputStream == null)
        {
            ByteArrayOutputStream os = new ByteArrayOutputStream();
            ServletInputStream superInputStream = super.getInputStream();
            int b = superInputStream.read();
            while (b != -1)
            {
                os.write(b);
                b = superInputStream.read();
            }
            inputStreamContent = os.toByteArray();
            inputStream = new ByteArrayServletInputStream(inputStreamContent);
        }
        return inputStream;
    }
}</code>
  </pre>
</div>

</p>
<p>
The response recorder was a bit trickier,  because I needed to save things like status codes and content types.  This implementation probably wouldn&#8217;t work for all clients (for example, it ignores any response headers), but since Gliffy is an OpenLaszlo app, and OpenLaszlo has almost <b>no</b> view into HTTP, this worked well for our purposes.  Again, I had to wrap the <tt>OutputStream</tt>/<tt>Writer</tt> so I could record what was being sent back.
<div>
  <pre>
    <code class='java'>private class RecordingServletResponse extends HttpServletResponseWrapper
{
    public RecordingServletResponse(HttpServletResponse r)
    {
        super(r);
    }

    int statusCode;
    StringWriter stringWriter = null;
    ByteArrayOutputStream byteOutputStream = null;
    String contentType = null;

    private PrintWriter writer = null;
    private ServletOutputStream outputStream = null;

    public ServletOutputStream getOutputStream()
        throws IOException
    {
        if (outputStream == null)
        {
            byteOutputStream = new ByteArrayOutputStream();
            outputStream = new RecordingServletOutputStream(super.getOutputStream(),new PrintStream(byteOutputStream));
        }
        return outputStream;
    }

    public PrintWriter getWriter()
        throws IOException
    {
        if (writer == null)
        {
            stringWriter = new StringWriter();
            writer = new RecordingPrintWriter(super.getWriter(),new PrintWriter(stringWriter));
        }
        return writer;
    }

    public void sendError(int sc)
        throws IOException
    {
        statusCode = sc;
        super.sendError(sc);
    }

    public void sendError(int sc, String msg)
        throws IOException
    {
        statusCode = sc;
        super.sendError(sc,msg);
    }

    public void setStatus(int sc)
    {
        statusCode = sc;
        super.setStatus(sc);
    }

    public void setContentType(String type)
    {
        contentType = type;
        super.setContentType(type);
    }
}</code>
  </pre>
</div>

</p>
<p>
The filter then needs to use this and inject them into the actual servlet calls:
<div>
  <pre>
    <code class='java'>public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
    throws IOException, ServletException
{
    RecordingServletRequest recordingRequest = 
      new RecordingServletRequest((HttpServletRequest)request);
    RecordingServletResponse recordingResponse = 
      new RecordingServletResponse((HttpServletResponse)response);

    chain.doFilter(recordingRequest,recordingResponse);</code>
  </pre>
</div>

After the call to <tt>doFilter</tt>, we can then examine the proxy request/respons and record the test.  I&#8217;ll spare you 20 lines of <tt>setXXX</tt> methods.  I created a Java bean class and used XStream to serialize it.  I then created another class that runs as a TestNG test to deserialize these files and make the same requests.  I record the response and see if it matches.
</p>
<h3>Running the Tests</h3>
<p>
There were a few problems with this approach:
<ul>
<li>The tests required certain test data to exist</li>
<li>Each test potentially modifies the database, meaning the tests have to be run in the order they were created.</li>
<li>The test results had temporal data in them that, while irrelevant to the tests &#8220;passing&#8221;, complicated exact-match comparisions of results</li>
</ul>
TestNG (and JUnit) are not really designed for this; they are more for proper <i>unit</i> testing, where each test can be run indepedent of the others and the results compared.  While there are facilities for setting up test data and cleaning up, the idea of resetting the database before each of the 300 tests I would record was not appealing.  Faking/mocking the database was not an option; I was creating these tests specifically to make sure my changes to the database layer were not causing regressions.  I needed to test against a real database.
</p>
<p>
I ultimately decided to group my tests into logical areas, and ensure that: a) tests were run in a predictable order, and b) the first test of a group was run against a known dataset.  I created a small, but useful, test dataset and created a TestNG test that would do both (a) and (b).  It wasn&#8217;t pretty, but it worked.  This clearly isn&#8217;t the way a unit test framework should be used, and  I would call these sorts of tests <i>functional</i>, rather than <i>unit</i>.  But, since our CI system requires JUnit test results as output, and the JUnit format isn&#8217;t documented, might as well use TestNG to handle it for me.
</p>
<p>
The last problem was making accurate comparisons of results.  I <b>did not</b> want to have to parse the XML returned by the server.  I settled on some regular expressions that stripped out temporal and transient data not relevant to the test.   Both the expected and received content were run through this regexp filter and <b>those</b> results were compared.  Parsing the XML might result in better failure messages (right now I have to do a visual diff, which is a pain), but I wasn&#8217;t convinced that the existing XML diff tools were that useful.
</p>
<h3>Results</h3>
<p>
Overall, it worked out great.  I was able to completely overhaul the database layer, and the Gliffy client was none the wiser.  We were even able to use these tests to remove our dependence on Struts, simplifying the application&#8217;s deployment (we weren&#8217;t using many features of Struts anyway).  The final validation of these tests actually came recently, when we realized a join table needed to be exposed to our server-side code.  This was a major change in two key data container, and the recorded tests were crucial to finding bugs this introduced.
</p>
<p>
So, if you don&#8217;t have the luxury of automated tests, you can always create them.  I did a similar thing with EJB3 using the <tt>Interceptors</tt> concept.
</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/06/01/daily-backups-are-gonna-save-my-butt.html">Daily Backups Are Gonna Save My Butt</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-06-01T00:00:00-04:00" pubdate data-updated="true">Jun 1<span>st</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content">I used to never back up.  Well, I&#8217;d throw some iTunes songs on a DVD every once in a while, but that&#8217;s about it.  Then, I started doing some pro audio work for friend&#8217;s bands and figured it was time to get serious.  My home computer, though, never really got the treatment.  When I upgraded to Leopard, I reformatted an external drive for Time Machine, but I wanted to have something better, as a double-check on Time Machine.  So, I did what I do for pro audio, which is to get a Firewire drive the exact size of my main drive, and use <a href="http://www.shirt-pocket.com/SuperDuper/SuperDuperDescription.html">Super Duper!</a> to mirror the drive every night, leaving the resulting drive bootable.  A plain <tt>rsync</tt> won&#8217;t work, for some reason; the drive gets duped, but isn&#8217;t bootable.

So, last night, I made the mistake of putting an old CD-R with a sticker on it into my slot-loading iMac (my main computer).  Now, Apple needs to <b>abandon this god-forsaken idiotic design decision</b> that is literally designed to play <b>Russian roulette</b> every time you stick a disc in.  The one thing I will give Windows over Mac: when you put a disc in a Windows box, you have a 100% chance of getting back out (and a 99% chance that doing will result in a bootable computer).  Well, this disc wouldn&#8217;t mount and wouldn&#8217;t eject.  A reboot of my computer resulted in&#8230;nothing.  Gray screen forever.  FUCK YOU APPLE.  Would it really have been so bad to have a button?  A tray that sticks out?  Ugh.  So, now I have to take my iMac to the Apple store to <b>pray to Jobs</b> that they can get the CD out.  Meanwhile, I&#8217;ve got work to do.

So, I grab my Macbook Pro that I use for Pro Audio, plug in my trusty firewire mirror drive, boot and&#8230;viola!  It&#8217;s like I never left?  Thankfully, the only thing I&#8217;ve done on my computer today was check email and surf the web and since those things are, you know, still on the Internet, I&#8217;m back.  I don&#8217;t know how I&#8217;m going to sync things back up when I get my box back, but I am thanking GOD right now that I do nightly backups (and that I have another computer to fall back on).  I guess when working from home, it&#8217;s good to have a spare.
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/05/14/using-threadlocal-and-servlet-filters-to-cleanly-access-jpa-an-entitymanager.html">Using ThreadLocal and Servlet Filters to Cleanly Access JPA an EntityManager</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-05-14T00:00:00-04:00" pubdate data-updated="true">May 14<span>th</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content">My current project is slowly moving from JDBC-based database interaction to JPA-based.  Following good sense, I&#8217;m trying to change things as little as possible.  One of those things is that we are deploying under Tomcat and not under a full-blown J2EE container.  This means that EJB3 is out.  After my <a href="http://www.naildrivin5.com/daveblog5000/?p=37">post regarding this configuration</a>, I quickly realized that my code started to get littered with:

<div>
  <pre>
    <code class='java'>EntityManager em = null;
try
{
  em = EntityManagerUtil.getEntityManager();
  // do stuff with entity manager
}
finally
{
  try {
    if (em != null) em.close();
  } catch (Throwable t) {
    logger.error(&quot;While closing an EntityManager&quot;,t);
  }
}</code>
  </pre>
</div>


Pretty ugly, and seriously annoying to have to add <b>13 lines of code</b> to any method that needs to interact with the database.  The Hibernate docs suggest using <tt>ThreadLocal</tt> variables to provide access to the EntityManager throughout the life of a request (which wouldn&#8217;t really work for a Swing app, but since this is servlet-based, it should work fine).    The <tt>ThreadLocal</tt> javadocs contain possibly the most annoying example ever, and I didn&#8217;t follow how to use it.

Anyway, I finally got around to it, and also solved the close problem as well, by using a Servlet Filter.  I guess this type of thing would normally be solvable by Spring or Guice, but I didn&#8217;t want to drag all of that into the application to refactor this one thing; I would&#8217;ve easily spent the rest of the day dealing with XML confihuration and deployment.

The solution was quite simple:

<div>
  <pre>
    <code class='java'>/** Provides access to the entity manager.  */
public class EntityManagerUtil 
{
    public static final ThreadLocal&amp;lt;EntityManager&gt; 
        ENTITY_MANAGERS = new ThreadLocal&amp;lt;EntityManager&gt;();

    /** Returns a fresh EntityManager */
    public static EntityManager getEntityManager()
    {
        return ENTITY_MANAGERS.get();
    }
}</code>
  </pre>
</div>


<div>
  <pre>
    <code class='java'>public class EntityManagerFilter implements Filter
{
    private Logger itsLogger = Logger.getLogger(getClass().getName());
    private static EntityManagerFactory theEntityManagerFactory = null;

    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
        throws IOException, ServletException
    {
        EntityManager em = null;
        try
        {
            em = theEntityManagerFactory.createEntityManager();
            EntityManagerUtil.ENTITY_MANAGERS.set(em);
            chain.doFilter(request,response);
            EntityManagerUtil.ENTITY_MANAGERS.remove();
        }
        finally
        {
            try 
            { 
                if (em != null) 
                    em.close(); 
            }
            catch (Throwable t) { 
                itsLogger.error(&quot;While closing an EntityManager&quot;,t); 
            }
        }
    }
    public void init(FilterConfig config)
    {
        destroy();
        theEntityManagerFactory = 
          Persistence.createEntityManagerFactory(&quot;gliffy&quot;);
    }
    public void destroy()
    {
        if (theEntityManagerFactory != null)
            theEntityManagerFactory.close();
    }
}</code>
  </pre>
</div>


So, when the web app gets deployed, the entity manager factory is created (and closed when the web app is removed).  Each thread that calls <tt>EntityManagerUtil</tt> to get an EntityManager gets a fresh one that persists for the duration of the request.  When the request is completed, the entity manager is closed automatically.
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/05/09/time-machine-almost-saved-me-but-git-won-out-in-the-end.html">Time Machine Almost Saved Me, but Git Won Out in the End</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-05-09T00:00:00-04:00" pubdate data-updated="true">May 9<span>th</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content">So, I&#8217;m working on a project that&#8217;s using Subversion for version control.  My network connection isn&#8217;t great, plus subversion is slow, plus git is (so far) pretty awesomely awesome. The way to interact with an SVN repository is via <tt>git-svn</tt>, <a href="http://www.naildrivin5.com/daveblog5000/?p=36">that I talked about setting up previously</a>.  Everything&#8217;s been going great, however I don&#8217;t frequently commit to subversion.  This week, we started setting up continuous integration for my work, so I did an <tt>git-svn dcommit</tt>, committing two days worth of changes.  I had forgotten that I had made so many changes (including adding <a href="http://www.naildrivin5.com/daveblog5000/?p=37">hibernate support</a>).  I misread the commit messages and thought something bad was happening.  Control-C.  <tt>git log</tt>.  HEAD is recent.  Last commit was&#8230;.yesterday.  Oh. Fuck.

I figure <tt>git-svn</tt> borked something, so I <tt>git-rest &#8211;hard</tt>.  No effect.  I&#8217;m starting to panic, now.  almost 2 days of work lost is not something I&#8217;m looking forward to.  I hasitly go into Time Machine and get the previous hours&#8217; backup.  But, I just <b>hate</b> that solution.  I have no idea what happened, and my trust in Git (or my ability to use it) has to be restored.  After IM&#8217;ing with a co-worker, I got to the bottom of it.

It turns out that I wasn&#8217;t paying attention to how <tt>git-svn</tt> works.  What it does when you do a <tt>rebase</tt> or <tt>dcommit</tt> (which implicitly does a <tt>rebase</tt>), is to first undo all your changes since your last <tt>rebase</tt>/<tt>dcommit</tt>, and get the changes made to the SVN repository (it even says as much as the first line of the output).  It then &#8220;replays&#8221; your commits to make sure there&#8217;s no conflicts.

By hitting Control-C in the middle of that, I manually caused the same situation that would happen if there were conflicts.  Git stops, tells you to resolve conflicts, and asks you to <tt>git-rebase &#8211;continue</tt>.  If I had just <tt>git-rebase &#8211;continue</tt>&#8216;ed, I would be fine. Since I did a hard rest, I figured I was fucked.  Enter the log.

<tt>.git/logs/HEAD</tt> contained information about all activity, including my missing commits.  I grab the version numbers (which, in Git, are hashes of the entire repository), do a <tt>git-reset &#8211;hard big.honkin.git.hash.version</tt> and <b>viola!</b> everything&#8217;s back to how it was (the command ran instanteously, to boot).

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/05/08/using-java-persistence-with-tomcat-and-no-ejbs.html">Using Java Persistence With Tomcat and No EJBs</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-05-08T00:00:00-04:00" pubdate data-updated="true">May 8<span>th</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content">The project I&#8217;m working on is deployed under Tomcat and isn&#8217;t using EJBs.  The codebase is using JDBC for database access and I&#8217;m looking into using some O/R mapping.  Hibernate is great, but Java Persistence is more desirable, as it&#8217;s more of a standard.  Getting it to work with EJB3 is dead simple.  Getting it to work without EJB was a bit more problematic.

The entire application is being deployed as a WAR file.  As such, the JPA configuration artifacts weren&#8217;t getting picked up.  Setting aside how absolutely horrendous Java Enterprise configuration is, here&#8217;s what ended up working for me:

<ul>
<li>Create a <tt>persistence.xml</tt> file as per standard documentation <b>leaving out the <tt>jta-data-source</tt> stanza</b> (I could not figure out how to get Hibernate/JPA to find my configured data source)</li>
<li>Create your <tt>hibernate.cfg.xml</tt>, being sure to <b>include JDBC conncetion info</b>.  This will result in hibernate managing connections for you, which is fine</li>
<li>Create a persistence jar containing:
    <ul>
    <li>Hibernate config at root</li>
    <li><tt>persistence.xml</tt> in <tt>META-INF</tt></li>
    <li>All classes with JPA annotations in root (obviously in their java package/directory structure)</li>
    </ul></li>
<li>This goes into <tt>WEB-INF/lib</tt> of the war file (being careful to omit the JPA-annotated classes from <tt>WEB-INF/classes</tt></li>
</ul>

The first two steps took a while to get to and aren&#8217;t super clear from the documentation.

To use JPA, this (non-production quality) code works:

<div>
  <pre>
    <code class='java'>EntityManagerFactory emf = 
    Persistence.createEntityManagerFactory(&quot;name used in persistence.xml&quot;);
EntityManager em = emf.createEntityManager(); 

Query query = em.createQuery(&quot;from Account where name = :name&quot;);
query.setParameter(&quot;name&quot;,itsAccountName);
List results = query.getResultList();

// do stuff with your results

em.close();
emf.close();</code>
  </pre>
</div>


The <tt>EntityManagerFactory</tt> is supposed to survive the life of application and not be created/destroyed on every request.

I also believe there might be some transaction issues with this, but I can&#8217;t figure out from the documentation what they are and if they are a big deal for a single-database application.

<b>Update</b>:  Turns out, it&#8217;s not quite this simple.  Since this configuration is running outside an EJB container, and given <a href="http://opensource.atlassian.com/projects/hibernate/browse/HHH-2382">Bug $2382</a>, you can query all day long, but you cannot persist.  To solve this, you must work in a transaction, as so:

<div>
  <pre>
    <code class='java'>EntityManagerFactory emf = 
    Persistence.createEntityManagerFactory(&quot;name used in persistence.xml&quot;);
EntityManager em = emf.createEntityManager(); 
EntityTransaction tx = em.getTransaction();

tx.begin();
Query query = em.createQuery(&quot;from Account where name = :name&quot;);
query.setParameter(&quot;name&quot;,itsAccountName);
List results = query.getResultList();

// modify your results somehow via persist() 
// or merge()

tx.commit();
em.close();
emf.close();</code>
  </pre>
</div>


Again, this is <b> not production code</b> as <b>no</b> error handling has been done at all, but you get the point.
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/04/28/git-and-svn-connecting-git-branches-to-svn-branches.html">Git and SVN: Connecting Git Branches to Svn Branches</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-04-28T00:00:00-04:00" pubdate data-updated="true">Apr 28<span>th</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content">Currently working on a project where Subversion is the CM system of choice.  I&#8217;d like to use git, as it&#8217;s faster and doesn&#8217;t require so much network access.  Plus, I&#8217;m hoping when it comes time to merge, I can simplify the entire process by using git&#8217;s allegedly superior merging technique.  At any rate, I&#8217;ve got a branch on SVN to work on, and I want to track both that branch and the entire svn tree.

Saturday morning, I did a <tt>git-svn init</tt> from their repository.  Today, after lunch, it finished.  After doing a <tt>git-gc</tt> to clean up the checkout, it wasn&#8217;t clear how to connect branches.  Following is what I did (assume my subversion branch is <tt>branches/FOO</tt>):

<blockquote><pre>
git-checkout -b local-trunk trunk
git branch local-foo FOO
</pre></blockquote>

The first thing creates a new branch called &#8220;local-trunk&#8221; started at &#8220;trunk&#8221; (which is the remote branch mapping to the subversion main trunk).  The second command creates a new branch called &#8220;local-foo&#8221;, which is rooted at remote branch &#8220;FOO&#8221;.  I have no clue why I couldn&#8217;t do the same thing twice, as both commands seem to do the same thing (the first switches to the branch &#8220;local-trunk&#8221; after creating it).  But, this is what worked for me.

Now, to develop, I <tt>git checkout local-foo</tt> and commit all day long.  a <tt>git-svn dcommit</tt> will send my changes to subversion on the FOO branch.  I can update the trunk via <tt>git checkout local-trunk</tt> and <tt>git-svn rebase</tt>.  My hope is that I can merge from the trunk to my branch periodically and then, when my code is merged to the trunk, things will be pretty much done and ready to go.  We&#8217;ll see.

On a side note, the git repository, which contains <b>every revision of every file in the subversion repository</b> is 586,696 bytes.  The subversion checkout of <b>just the FOO branch</b> is 1,242,636 bytes; over double the size, and there&#8217;s still not enough info in that checkout to do a log or diff between versions.
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/04/21/rest-security-signing-requests-with-secret-key-but-does-it-work.html">REST Security: Signing Requests With Secret Key, but Does It Work?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-04-21T00:00:00-04:00" pubdate data-updated="true">Apr 21<span>st</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content">Both <a href="http://docs.amazonwebservices.com/AmazonS3/2006-03-01/gsg/?ref=get-started">Amazon Web Services</a> and the <a href="http://www.flickr.com/services/api/auth.howto.web.html">Flickr Services</a> provide <a href="http://en.wikipedia.org/wiki/Representational_State_Transfer">REST</a> APIs to their services.  I&#8217;m currently working on developing such a service, and noticed that both use signatures based on a shared secret to provide security (basically using a <a href="http://en.wikipedia.org/wiki/HMAC">Hash Message Authentication Code</a>).

It works as follows:
<ol>
<li>Applications receive a shared secret known only to them and the service provider.</li>
<li>A request is constructed (either a URL or a query string)</li>
<li>A digest/hash is created using the shared secret, based on the request (for Flickr, the parameter keys and values are assembled in a certain way, so that Flickr can easily generate the same string)</li>
<li>The digest is included in the request</li>
<li>The service provider, using the shared secret, creates a digest/hash on the request it receives</li>
<li>If the service provider&#8217;s signature matches the one included in the request, the request is serviced</li>
</ol>

It&#8217;s actually quite simple, and for one-time requests, is effective.  The problem, however, is that anyone intercepting the request can make it themselves, without some other state being shared with the client and service provider.  Consider a request for an image.  The unsigned request might look like:

<code>http://www.naildrivin5.com/api/images?image_id=45&type=jpg</code>

The signed request, would look like so:

<code>http://www.naildrivin5.com/api/images?image_id=45&type=jpg&signature=34729347298473</code>

So, anyone can then take that URL and request the resource.  They don&#8217;t need to know the shared secret, or the signature algorithm.  This is a bit of a problem.  One of the advantages of REST is that URLs that request resources are static and can be cached (much as WWW resources are).  So, if I wish to protect the given URL, how can I do so?

<h3>HTTP Authentication</h3>

The usual answer is HTTP Authentication; the service provide protects the resource, and the client must first log in.  Login can be done programmatically, and this basically accomplishes sending a second shared secret with the request that cannot be easily intercepted.  HTTP Auth has its issues, however, and might not be feasible in every context.

Another way to address this is to provide an additional piece of data that makes each request unique and usable only once.  To do so requires state to be saved on the client and the server.

<h3>Negotiated One-time Token</h3>

Authentication can be avoided by using the shared secret to establish a token, usable for one request of the given resource.  It would work like this:
<ol>
<li>Client requests a token for a given resource</li>
<li>Service Provider creates a token (via some uuid algorithm ensuring no repeats) and associates it with the resource</li>
<li>Client creates a second request, as above, for the resource, including the token in the request</li>
<li>Service Provider checks not just for a valid signature, but also that the provided token is associated with the given resource</li>
<li>If so, the token is retired, and the resource data is returned</li>
</ol>

Here, the URL constructed in step 3 can be used only once.  Anyone intercepting the request can&#8217;t make it again, without constructing a new one, which they would be unable to do without the shared secret.  Further, this doesn&#8217;t preclude caching.  The main issue here is that since two requests are required, simultaneous access to one resource could result in false errors: if Client A acquires a token, and Client B requests one before Client A <b>uses</b> the token, Client A&#8217;s token could be squashed, resulting in an error when he makes his request.  The service provider can alleviate this by allowing the issuance of multiple active tokens per resource.

<h3>Timestamp</h3>

A disadvantage to the One-Time Token method is that it requires two requests of the service provider for every actual request (one to get the token and one to request the resource).  A way around that is to include a timestamp in the request.  This would work as follows:

<ol>
<li>Client creates request, including the current time.  This request is signed as per above procedure</li>
<li>Service provider validates the request and compares it&#8217;s time with the given timestamp.</li>
<li>If the difference in the service provider&#8217;s time and the client&#8217;s provided time is within some tolerance, the request is serviced</li>
</ol>

This obviously requires the two clocks to be vaguely in sync.  It also allows the resource to be requested by anyone within the timespan of the tolerance.  But, it does save a second request to the client.

<h3>Self-created One-time Token</h3>

This is an amalgam of the Timestamp solution and the Negotiated One-time Token solution.  Here, the client creates its own token, as a simple integer of increasing value.  The server maintains the last requested value and accepts only requests with a higher number:

<ol>
<li>Client creates request, using a global long-lived number</li>
<li>Client signs requests and sends it to the service provider</li>
<li>Service provider validates the signature and compares the provided numeric token with the one last used (the tokens can be globally scoped, or scoped for a given resource)</li>
<li>If the provided numeric token is greater than the previous, the request is serviced</li>
<li>The Client increments his numeric token for next time</li>
</ol>

As with the Timestamp solution, only one request is required.  As with the negotiated one-time token solution, the URL can never be used twice.  The main issue here is if the client forgets its numeric token.  This could be addressed with an additional call to re-establish the token, made only when the Client has determined it no longer knows the last used value.

Unfortunately, this is much more susceptible to race conditions than the Negotiated one-time token.  Since the service provider doesn&#8217;t know what tokens to expect (only that they should be greater than the last requested one), the client has to ensure that the &#8220;create request, submit request, receive response, update local numeric token&#8221; cycle is atomic.  That is not straightforward.

<b>Update</b> Got another idea from a co-worker

<h3>Session Token</h3>

When a user access the system that uses the REST API, they get issued a token (via the REST API).  This token is just like a session token, with an inactivity timeout and so forth.  The token can be manually invalidated via the API, so that when a user logs out or completes some logical task, the token can be invalidated.

This suffers none of the problems of the other solutions, though it isn&#8217;t the most secure.  However, the security problem it has (using the valid URL before the session times out) is fairly minor, and the tradeoff of getting one request per actual request and no race conditions makes it probably the best way to go.
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/04/17/shell-history-meme.html">Shell History Meme</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-04-17T00:00:00-04:00" pubdate data-updated="true">Apr 17<span>th</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content">In response to the rage-of-the-moment <a href="http://raibledesigns.com/rd/entry/history_meme">shell history</a> meme, I present my list, which looks different than a lot of peoples&#8217;:
<ul>
<li><b>149</b><tt> cd</tt> - Well, duh</li>
<li><b>104</b><tt> ./run.sh</tt> - Occasionally to run JBoss (I have to have two JBoss servers running locally, and when I do a complete recompile/redeploy (not often), I must restart, as ant fstats the ever-loving shit out thousands of files (multiple times, per, &#8216;natch, cause it so superior to <tt>make</tt>) at the same time JBoss is hot-deploying (twice) and my machine just dies), but more often to run the application I&#8217;m developing.</li>
<li><b>88</b><tt> tail</tt> - To read log files (were I on OS X, Console.app would be the way to go)</li>
<li><b>84</b><tt> ls</tt> - Well, duh</li>
<li><b>73</b><tt> git</tt> - I <code>commit</code> a lot, I <code>diff</code> alot, I <code>stat</code> alot, and I <code>blame</code> alot.  <a href="http://www.naildrivin5.com/daveblog5000/?p=32">You should, too</a></li>
<li><b>57</b><tt> sqlshell.pl</tt> - Awesomely awesome command-line SQL client written by my co-worker and enhanced on occasion by me.  I&#8217;ll probably steal it when I leave this project, and I wish he&#8217;d put it up on sourceforge</li>
<li><b>51</b><tt> jobs</tt> - To find out which JBosses to kill (frequently executed in the wrong shell window)</li>
<li><b>44</b><tt> kill</tt> - To then go kill JBoss</li>
<li><b>43</b><tt> vi</tt> - To get some work done</li>
</ul>

Runners up: <code>rm</code>, <code>grep</code>, <code>git-svn</code>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/04/15/distributed-version-control-with-git-for-code-quality-and-team-organization.html">Distributed Version Control With Git for Code Quality and Team Organization</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-04-15T00:00:00-04:00" pubdate data-updated="true">Apr 15<span>th</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content">In <a href="http://www.naildrivin5.com/daveblog5000/?p=31">my previous post</a>, I outlined a code review process I&#8217;ve been using with reasonably effectiveness.  It&#8217;s supported, in my case, by <a href="http://git.or.cz">the Git</a> source code management tool (most known for it&#8217;s use in managing the Linux kernel).  Git or, more generally, distributed development, can encourage some good quality control procedures in teams working on enterprise software.  The lessons learned from the open source world (and the Linux kernel, in particular) can be applied outside the world of OSS and to the consultant-heavy world of enterprise/in-house software development.

The project I&#8217;ve been working on for the past several months has undergone what I believe to be a common change on in-house/enterprise software, which is that several new developers are being added to the project.  Outside of the learning curve required with any new system, many of them are not seasoned Java developers, or are otherwise missing experience in some key technologies in use.   While <a href="http://www.naildrivin5.com/daveblog5000/?p=31">code reviews</a> are a great way to ensure these developers are doing things the right way, there is still concern that their ability to commit to source control could be problematic for the entire team.

Consider a developer breaking the build, or incorrectly refactoring a key piece of shared code.  A review of their commit and some continuous integration can help identify these problems, but, once identified, they must be removed from the codebase.  In the meantime, the development team could be stuck with an unusable build.  This can lead to two bad practices:
<ul>
<li>Commit very rarely</li>
<li>Get new changes from the repository only when absolutely  needed</li>
</ul>
These &#8220;anti-practices&#8221; result in unreadable commit logs, difficult (or skipped) code reviews, duplication of code, and a general discoherence of the system.  This is primarily due to the way most common version control systems work. 

In reserved-checkout systems (e.g. PVCS, StarTeam) <strong>and</strong> concurrent systems (CVS, Subversion), there is the concept of the <b>one true repository of code</b> that is a bottleneck for all code on the project.  The only way Aaron can use Bill&#8217;s code is for Bill to commit it to the repository and for Aaron to check it out (along with anything else committed since the last time he did so).  The only way Carl can effectively review Dan&#8217;s code, or for the automated build to run his test cases, is to checkout code from the repository and examine/run it.  <b>This</b> reality often leads to situations where each developer is operating on his own branch.  The problem here is that CVS and Subversion suck at merging.  This makes the branching solution effectively useless.

Enter Git.  With Git, there is no central repository.  Each developer is on his own branch (or his own copy of someone&#8217;s branch) and can commit to their heart&#8217;s content, whenever <b>they</b> feel they have reached a commit point.  Their changes will <b>never</b> be forced upon the rest of the team.  So, how does the code get integrated?

Developer&#8217;s submit their code to the team lead/integrator (who is the ultimate authority on what code goes to QA/production/the customer), who then reviews it and either accepts or rejects it.  If code is rejected, the team lead works with the developer to get it accepted (either via a simple email of the issues, or more in-depth mentoring as needed).  Git makes this painless and fast, because it handles merging so well.

Consider how effective this is, especially when managing a large (greater than, say, five) team of developers working concurrently.  The only code that gets into the production build will have been vetted through the team lead; he is responsible for physically applying each developer&#8217;s patches (an action that takes a few minutes or even seconds in Git).  Further, developers get instant feedback on their code quality.  In most cases, bad commits are the result of ignorance and lack of experience.  A code review, with instant feedback, is a great way to address both of those issues, resulting in a better developer and a better team, based on open, honest, and immediate communication.

Here&#8217;s how to set this up:
<ol>
<li><b>Assign a team lead to integrate the code</b> - this is a senior <i>developers</i> who can assess code quality, provide mentoring and guidance and can be trusted to put code into the repository destined for QA and production</li>
<li><b>Each developer clones the team lead&#8217;s repository</b> - This is done to baseline the start of their work</li>
<li><b>Developers commit, branch, merge, and pull as necessary</b> - Since Git makes merging simple, developer&#8217;s can have full use of all features of version control and can do so in their environment without the possibility of polluting the main line of development.  They can also share code amongst themselves, as well as get updates from the team lead&#8217;s repository of &#8220;blessed&#8221; code<sup><a href="#1">1</a></sup></li>
<li><b>Developer&#8217;s inform the lead of completion</b></li>
<li><b>Lead pulls from their repository</b> - The lead reviews the developer&#8217;s changes and applies the patch to his repository.  He can then exercise whatever quality control mechanisms he wishes, including automated tests, manual tests, reviews, etc<sup><a href="#2">2</a></sup>.
<li><b>Lead rejects patches he doesn&#8217;t agree with</b> - If the patch is wrong, buggy, or just not appropriate in some way, the lead rejects the patch and provides the developer with information on the correct approach</li>
<li><B>Lead accepts patches he does agree with</b> - If the lead agrees with the patch, he applies it to his repository, where it is now cleared for QA</li>
</ol>

This may seem convoluted, but it actually carries little overhead compared to a junior developer performing a &#8220;nuclear bomb&#8221; commit that must then be rolled back.  For much larger teams, the approach can be layered, with the primary team lead accepting patches only from lieutenants, who accept patches from the primary developers.

Unlike a lot of hand-wavy processes and practices, this model has been demonstrated effective on virtually <b>every open source project</b>.  Even though the Linux kernel is one of the few to use technology to support this process (Git), every other large OSS project has the concept of &#8220;committers&#8221; who are the people allowed to actually commit.  Anyone else wishing to contribute must submit patches to a committer, who then reviews and approves of their patch (or not).

I belive this would be highly effective in a professional environment developing in-house or enterprise software (especially given the typical love of process in those environments; <b>this</b> process might actually help!).  I have been on at least three such projects where it would&#8217;ve been an enormous boon to quality (not to mention that the natural mentoring and feedback built into the process would&#8217;ve been hugely helpful for the more junior developers).

<hr />
<a name="1"><b>1</b></a> <i>Git even allows a developer to merge certain <b>commits</b> from one branch to another.  Suppose Frank is working on a large feature, and happens to notice a bug in common code.  He can address that bug and commit it.  Gary can then merge only that commit into his codebase to get the bugfix, without having to also take all of Frank&#8217;s in-progress work on the large feature.  Good luck doing that with StarTeam.</i><br />
<a name="2"><b>2</b></a> <i> A CI system could be set up in a variety of ways:  it could run only against the lead&#8217;s &#8220;blessed&#8221; repository, or it could run against an intermediate repository created by the lead (who then blesses patches that pass), or it could be totally on its own and allow developers to submit against it prior to submitting to the lead.
</i>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/04/03/quick-and-dirty-code-reviews-check-commit-logs.html">Quick and Dirty Code Reviews: Check Commit Logs</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2008-04-03T00:00:00-04:00" pubdate data-updated="true">Apr 3<span>rd</span>, 2008</time>
        
      </p>
    
  </header>


  <div class="entry-content"><blockquote><pre>             Large maintenance 
+          aggressive schedule 
+       lots of new developers 
+ minimal system documentation
______________________________
 Need for highly efficient and 
       effective QA procedures</pre></blockquote>

Where I&#8217;ve been working for the past few months, we&#8217;ve been under the gun to meet an aggressive deadline.  As management is want to do, they&#8217;ve added several new developers to the project.  One of the many reasons why adding developers is ultimately a bad thing is that, in addition to the complexity in communication, there is a risk of innocent well-developed code being added to the codebase that is Just Plain Wrong.  Our system has been in development for many years and contains a wide variety of coding styles, implementation patterns and idioms.  Some of them should Never Be Followed Again, and some are the Correct Way of Doing Things.   There&#8217;s really no easy way for a new developer to know what is what.

Now, outside of going back in time, creating pedantic test cases, gathering requirements and incessantly refactoring, we need an option to make sure bad code doesn&#8217;t get into the codebase.  By &#8220;bad&#8221; I don&#8217;t mean poorly written or buggy code, I mean code that does not fit into the system as a whole.  For example, a developer was writing some code to generate printable reports.  His implementation resulted in a very nice report popping up in a Swing window in our application (our application is a Swing front-end to a JEE back-end).  It was very well-implemented and looked great.  However, everywhere else in the application, reports are generated by writing a PDF to disk and asking Windows (via JDIC) to &#8220;view&#8221; the file.  This is the code I&#8217;m talking about.

<h3>Finding bad code</h3>
At the start of the project, we went through the arduous process of identifying each bit of work in an MS-Project file and assigning developers to it.  New developers were given tasks that didn&#8217;t touch core components, while experienced developers got tasks involving major refacotrings or database changes.  Our project lead suggested that each module undergo a code review.  It sounds reasonable, however all of us let out a collective groan at the thought of wrangling together 3-4 developers once a week for an hour or two to go over printouts or screenfulls of code, much of which was simply being modified.

One of the senior developers proposed the solution we ultimately went with: senior developers get emailed the diffs of all commits and make sure to spend some time reading and reviewing those commits.  Coupled with our policy of &#8220;commit early, commit often&#8221;, this has worked out great.

<h3>Diff-based code review</h3>

Here&#8217;s what you need:

<ul>
<li>A concurrent version control system developers trust.  Recommed <a href="http://git.or.cs">Git</a> or <a href="http://subversion.tigris.org">Subversion</a> if you must.</li>
<li>A simple script to email diffs on <b>every</b> commit.  Usually included as an example hook for must version control systems.</li>
<li>IM clients (Google talk within GMail works in even the most oppressive environment)</li>
<li>A sane version control policy: committed code must:
    <ul>
    <li>Compile</li>
    <li>Not prevent application deployment/startup</li>
    <li>Not horribly break someone else&#8217;s code (optional)</li>
    </ul>
    Developers should commit as frequently as they want (and preferably frequently).  I typically commit code I feel is &#8220;done&#8221; but that might not add up to an actual feature.  This requires accepting that <b>head is not a real verison</b>.  Most real version control systems have the ability to tag, branch, etc.  These features are for &#8220;real working versions&#8221;.  The head of the trunk is not.</li>
<li>A sane coding style policy: if you must re-indent or change bracing style, do it in its own commit, outside of actual code changes.  Better yet, don&#8217;t do it at all.  Formatting changes can obscure the history of a piece of code and should be made minimally, if at all. </li>
</ul>

The &#8220;process&#8221; (if you want to even call it that) is:

<ol>
<li>Diffs get emailed to the senior developers as soon as they happen</li>
<li>Senior Developers read the diffs, using IM to discuss any issues</li>
<li>If code does have issues, the diff is forwarded to the developer who committed, with comments on what to change and why (senior developers decide amongst themselves who will send the feedback, or a &#8220;lead developer&#8221; can, if one is identified)</li>
</ol>

Part of this requires some level of diplomacy, however a plain, to-the-point email on what the issues are with a piece of code, why the changes should be made, and a suggestion on how to make them should be digestible by anyone.

I&#8217;ve had great success with this, having caught a wide variety of problems (even in my code, by others) without having to have <b>one</b> meeting or to print out <b>one</b> sheet of code.  The fact is, on a maintenance project, you aren&#8217;t reviewing the codebase, but <b>changes</b> to that codebase.  Diffs are the way to understand what changes are being made.
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/10/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/8/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>
  <img class="left" src="http://www.gravatar.com/avatar/ae52d95bbc43d0a62044a9a6b5674de0.png">
  I&#8217;ve been a progammer since 1995.  Currently use Ruby and Scala.
  I also write, play bass, and cook.
  </p>
</section>
<section>
  <p>
  <a href="http://pragprog.com/book/dccar/build-awesome-command-line-applications-in-ruby"><img class="center" src="http://imagery.pragprog.com/products/249/dccar_xlargebeta.jpg?1319573406"></a>
  </p>
</section>

<section>
  <h1>Github Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/davetron5000">@davetron5000</a> on Github
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'davetron5000',
            count: 5,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("davetron5000", 5, true);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/davetron5000" class="twitter-follow-button" data-show-count="false">Follow @davetron5000</a>
  
</section>

<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/05/02/what-makes-and-awesome-command-line-app.html"> What Makes and Awesome Command-Line App?</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/04/24/five-months-of-ebook-sales.html">&#10144; Five Months of eBook Sales</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/04/11/methadone-1-dot-0-with-awesome-tutorial.html">Methadone 1.0 with Awesome Tutorial</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/04/05/i-heartily-endorse-ruby-off-rails.html">I heartily endorse Ruby off Rails</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/04/02/a-protocol-for-code-reviews.html">A Protocol for Code Reviews</a>
      </li>
    
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - David Bryant Copeland -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
